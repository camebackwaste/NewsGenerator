{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers==4.51.3 datasets==2.14.4 torch==2.6.0 ipywidgets==7.7.1 rouge_score\n"
      ],
      "metadata": {
        "id": "74PMZ6ttl2mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wu_w_izSzTlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "id": "78QpmKsql46T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "base_path = '/content/drive/MyDrive/BBC News Summary'\n",
        "articles_dir = os.path.join(base_path, 'News Articles')\n",
        "model_save_path = '/content/drive/MyDrive/News_Generator_Final'\n",
        "os.makedirs(model_save_path, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "DYpPJJMwl9nS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "from datetime import datetime\n",
        "\n",
        "def cargar_articulos(categoria):\n",
        "    articulos = []\n",
        "    for archivo in glob.glob(os.path.join(articles_dir, categoria, '*.txt')):\n",
        "        with open(archivo, 'r', encoding='latin-1') as f:\n",
        "            contenido = f.read().split('\\n')\n",
        "            titulo = contenido[0].strip()\n",
        "            cuerpo = ' '.join([linea.strip() for linea in contenido[1:] if linea.strip()])\n",
        "            articulos.append({\n",
        "                'category': categoria.upper(),\n",
        "                'title': titulo,\n",
        "                'content': cuerpo[:1800]\n",
        "            })\n",
        "    return articulos\n",
        "\n",
        "categorias = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
        "df = pd.DataFrame([art for cat in categorias for art in cargar_articulos(cat)])\n"
      ],
      "metadata": {
        "id": "AuRq8-WhmBGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime as dt\n",
        "hoy_str = dt.now().strftime('%Y-%m-%d')\n",
        "\n",
        "df['text'] = df.apply(\n",
        "    lambda x: f\"\"\"\\\n",
        "[{x['category']}]\n",
        "FECHA: {hoy_str}\n",
        "TITULAR: {x['title']}\n",
        "CONTENIDO: {x['content']}\n",
        "---\"\"\",\n",
        "    axis=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "mlLeohgOmU5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "def funcion_tokenizacion(ejemplos):\n",
        "    return tokenizer(\n",
        "        ejemplos['text'],\n",
        "        max_length=512,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "from datasets import Dataset\n",
        "dataset = Dataset.from_pandas(df[['text']])\n",
        "dataset = dataset.map(funcion_tokenizacion, batched=True, batch_size=32, remove_columns=['text'])\n",
        "\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n"
      ],
      "metadata": {
        "id": "R1aVfrv3mXbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "modelo = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
        "modelo.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "argumentos_entrenamiento = TrainingArguments(\n",
        "    output_dir='./resultados',\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "entrenador = Trainer(\n",
        "    model=modelo,\n",
        "    args=argumentos_entrenamiento,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "1j63eJfwnuvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" Iniciando entrenamiento...\")\n",
        "entrenador.train()\n"
      ],
      "metadata": {
        "id": "oDPrx5bZoBda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = \"/content/drive/MyDrive/News_Generator_Final\"\n",
        "\n",
        "entrenador.model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "uq7b38bzr2kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def generar_desde_gradio(categoria, longitud, temperatura):\n",
        "    prompt = f\"[{categoria}]\\nFECHA: {dt.now().strftime('%Y-%m-%d')}\\nTITULAR:\"\n",
        "    generado = generador(\n",
        "        prompt,\n",
        "        max_length=longitud,\n",
        "        temperature=temperatura,\n",
        "        top_p=0.92,\n",
        "        repetition_penalty=1.35,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2\n",
        "    )\n",
        "    contenido = generado[0]['generated_text'].split(\"CONTENIDO:\", 1)[-1].strip()\n",
        "    return contenido\n",
        "\n",
        "categorias_opciones = [c.upper() for c in categorias]\n",
        "\n",
        "gr.Interface(\n",
        "    fn=generar_desde_gradio,\n",
        "    inputs=[\n",
        "        gr.Dropdown(choices=categorias_opciones, label=\"CategorÃ­a\"),\n",
        "        gr.Slider(200, 800, step=50, value=400, label=\"Longitud\"),\n",
        "        gr.Slider(0.3, 1.2, step=0.05, value=0.72, label=\"Creatividad\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(lines=20, label=\"Noticia Generada\"),\n",
        "    title=\"ðŸ“° Generador de Noticias BBC\",\n",
        "    description=\"Modelo de texto entrenado con artÃ­culos BBC | DistilGPT2\"\n",
        ").launch(share=True)"
      ],
      "metadata": {
        "id": "FYuaxmDPMJy5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
